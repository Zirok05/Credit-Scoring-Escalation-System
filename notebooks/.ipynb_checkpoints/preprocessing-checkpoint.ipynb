{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4d1ae377-11ee-4f07-bb27-d5ca69f1b847",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "\n",
    "FOLDER_PATH = '/Users/artemzmailov/Desktop/GiveMeSomeCredit/'\n",
    "train_data_full = pd.read_csv(FOLDER_PATH + 'data/cs-training.csv', index_col = 0)\n",
    "train_label_full = train_data_full['SeriousDlqin2yrs']\n",
    "test_data_full = pd.read_csv(FOLDER_PATH + 'data/cs-test.csv', index_col = 0).drop(columns = ['SeriousDlqin2yrs'])"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a6018639-3975-4a32-b77d-3d2ac27c7ad6",
   "metadata": {},
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_data, test_data, train_label, test_label = train_test_split(\n",
    "    train_data_full,\n",
    "    train_label_full,\n",
    "    test_size = 0.2, \n",
    "    stratify = train_label_full,\n",
    "    shuffle = True,\n",
    "    random_state = 42)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "63d1b43b-641d-46d1-bb8e-8a0ae67a5b72",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "\n",
    "\n",
    "class CreditDataPreprocessor(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    # Полный препроцессинг данных\n",
    "    \n",
    "    def __init__(self,\n",
    "                 NumberOfDependents_fill_value = 0,\n",
    "                 NumberOfDependents_up_threshold = 10,\n",
    "                 MonthlyIncome_fill_value = 0,\n",
    "                 RevolvingUtilizationOfUnsecuredLines_drop_threshold = 2,\n",
    "                 age_low_drop_threshold = 18,\n",
    "                 age_up_drop_threshold = 80,\n",
    "                 DebtRatio_up_threshold = 5,\n",
    "                 PastDueRiskScore_weights = [1.0, 1.2, 1.3],\n",
    "                 NumberRealEstateLoansOrLines_drop_threshold = 20):\n",
    "\n",
    "        self.NumberOfDependents_fill_value = NumberOfDependents_fill_value\n",
    "        self.NumberOfDependents_up_threshold = NumberOfDependents_up_threshold\n",
    "        \n",
    "        self.MonthlyIncome_fill_value = MonthlyIncome_fill_value\n",
    "\n",
    "        self.RevolvingUtilizationOfUnsecuredLines_drop_threshold = RevolvingUtilizationOfUnsecuredLines_drop_threshold\n",
    "        \n",
    "        self.age_low_drop_threshold = age_low_drop_threshold\n",
    "        self.age_up_drop_threshold = age_up_drop_threshold\n",
    "\n",
    "        self.DebtRatio_up_threshold = DebtRatio_up_threshold\n",
    "\n",
    "        self.PastDueRiskScore_weights = PastDueRiskScore_weights\n",
    "\n",
    "        self.NumberRealEstateLoansOrLines_drop_threshold = NumberRealEstateLoansOrLines_drop_threshold\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "\n",
    "        X_copy = X.copy()\n",
    "        \n",
    "        X_copy['NumberOfDependents'] = X_copy['NumberOfDependents'].fillna(value = self.NumberOfDependents_fill_value)\n",
    "        X_copy['NumberOfDependents'] = X_copy['NumberOfDependents'].clip(0,self.NumberOfDependents_up_threshold).copy()\n",
    "\n",
    "        X_copy['MonthlyIncomeIsMissing'] = 0\n",
    "        X_copy.loc[X_copy['MonthlyIncome'].isna(), 'MonthlyIncomeIsMissing'] = 1\n",
    "        X_copy['MonthlyIncome'] = X['MonthlyIncome'].fillna(value = self.MonthlyIncome_fill_value)\n",
    "\n",
    "        X_copy['RevolvingUtilizationOverOne'] = 0.0\n",
    "        X_copy.loc[X_copy['RevolvingUtilizationOfUnsecuredLines'] > 1, 'RevolvingUtilizationOverOne'] = 1.0\n",
    "        X_copy['RevolvingUtilizationOfUnsecuredLines'] = X_copy['RevolvingUtilizationOfUnsecuredLines'].clip(0,1).copy()\n",
    "\n",
    "        X_copy['DebtPayments'] = 0.0\n",
    "        X_copy.loc[X_copy['MonthlyIncome'] == 0,'DebtPayments'] = X_copy.loc[X_copy['MonthlyIncome'] == 0,'DebtRatio']\n",
    "        X_copy.loc[X_copy['MonthlyIncome'] != 0,'DebtPayments'] = X_copy.loc[X_copy['MonthlyIncome'] != 0,'DebtRatio'] * X_copy.loc[X_copy['MonthlyIncome'] != 0,'MonthlyIncome']\n",
    "        X_copy['DebtRatio'] = X_copy['DebtRatio'].clip(0,self.DebtRatio_up_threshold).copy()\n",
    "\n",
    "        X_copy['DebtPayments_over_10k'] = 0.0\n",
    "        X_copy.loc[X_copy['DebtPayments'] > 10000,'DebtPayments_over_10k'] = 1.0\n",
    "        X_copy['DebtPayments'] = X_copy['DebtPayments'].clip(0,10000).copy()\n",
    "\n",
    "        X_copy['MonthlyIncome_over_20k'] = 0.0\n",
    "        X_copy.loc[X_copy['MonthlyIncome'] >= 20000,'MonthlyIncome_over_20k'] = 1.0\n",
    "        X_copy['MonthlyIncome'] = X_copy['MonthlyIncome'].clip(0,20000)\n",
    "\n",
    "        X_copy['Code96'] = 0.0\n",
    "        X_copy['Code98'] = 0.0\n",
    "        X_copy.loc[X_copy['NumberOfTime30-59DaysPastDueNotWorse'] == 96, 'Code96']  = 1.0\n",
    "        X_copy.loc[X_copy['NumberOfTime30-59DaysPastDueNotWorse'] == 98, 'Code98']  = 1.0\n",
    "\n",
    "        X_copy['PastDueRiskScore'] = (\n",
    "            self.PastDueRiskScore_weights[0] * X_copy['NumberOfTime30-59DaysPastDueNotWorse'] +\n",
    "            self.PastDueRiskScore_weights[1] * X_copy['NumberOfTime60-89DaysPastDueNotWorse'] +\n",
    "            self.PastDueRiskScore_weights[2] * X_copy['NumberOfTimes90DaysLate'])\n",
    "        X_copy.loc[X_copy['NumberOfTime30-59DaysPastDueNotWorse'] == 96, 'PastDueRiskScore'] = 96\n",
    "        X_copy.loc[X_copy['NumberOfTime30-59DaysPastDueNotWorse'] == 98, 'PastDueRiskScore'] = 98\n",
    "        X_copy = X_copy.drop(columns = ['NumberOfTime30-59DaysPastDueNotWorse', 'NumberOfTime60-89DaysPastDueNotWorse', 'NumberOfTimes90DaysLate'])\n",
    "\n",
    "        X_copy['NumberOfOpenCreditLinesAndLoans_over_30'] = 0.0\n",
    "        X_copy.loc[X_copy['NumberOfOpenCreditLinesAndLoans'] > 30, 'NumberOfOpenCreditLinesAndLoans_over_30'] = 1.0\n",
    "        X_copy['NumberOfOpenCreditLinesAndLoans'] = X_copy['NumberOfOpenCreditLinesAndLoans'].clip(0,30).copy()\n",
    "\n",
    "        X_copy['NumberRealEstateLoansOrLines_over_5'] = 0.0\n",
    "        X_copy.loc[X_copy['NumberRealEstateLoansOrLines'] > 5, 'NumberRealEstateLoansOrLines_over_5'] = 1.0\n",
    "        X_copy['NumberRealEstateLoansOrLines'] = X_copy['NumberRealEstateLoansOrLines'].clip(0,5).copy()\n",
    "        \n",
    "        X_copy['ConsumerCredit_Group'] = pd.cut(X_copy['NumberOfOpenCreditLinesAndLoans'], \n",
    "                                        bins = [0,1, 2,6,15,31], \n",
    "                                        labels=[\n",
    "                                            '0_loans',\n",
    "                                            '1_loans',\n",
    "                                            '2-5_loans',\n",
    "                                            '6-14_loans',\n",
    "                                            '16-30_loans'\n",
    "                                        ])\n",
    "        consumer_dummy = pd.get_dummies(X_copy['ConsumerCredit_Group'], prefix='Consumer', drop_first = False).astype('float')\n",
    "        \n",
    "        X_copy['RealEstateLoans_Group'] = pd.cut(X_copy['NumberRealEstateLoansOrLines'],\n",
    "                                           bins=[-1, 0, 3,100], \n",
    "                                           labels= [\n",
    "                                                    '0_loans',      \n",
    "                                                    '1-3_loans',    \n",
    "                                                    '4+_loans',    \n",
    "                                                    ])\n",
    "        estate_dummy = pd.get_dummies(X_copy['RealEstateLoans_Group'], prefix='RealEstateLoans', drop_first = False).astype('float')\n",
    "\n",
    "        X_copy = pd.concat([X_copy, consumer_dummy, estate_dummy], axis = 1).copy()\n",
    "        X_copy = X_copy.drop(columns = ['ConsumerCredit_Group', \n",
    "                                                'RealEstateLoans_Group']).copy()\n",
    "        \n",
    "        X_copy = X_copy.drop(columns = ['Consumer_6-14_loans',  \n",
    "                                                'RealEstateLoans_0_loans']).copy()\n",
    "        \n",
    "\n",
    "        X_copy = X_copy.drop(columns = ['NumberOfOpenCreditLinesAndLoans',\n",
    "                                                'NumberRealEstateLoansOrLines',\n",
    "                                               'MonthlyIncomeIsMissing',\n",
    "                                               'MonthlyIncome_over_20k',\n",
    "                                               'Consumer_0_loans',\n",
    "                                               'NumberOfOpenCreditLinesAndLoans_over_30']).copy()\n",
    "                \n",
    "        return X_copy\n",
    "    \n",
    "    def fit_transform(self, X, y=None):\n",
    "        return self.fit(X, y).transform(X)\n",
    "\n",
    "\n",
    "    def clean_train(self, X, y=None):\n",
    "        mask = (\n",
    "            (X['RevolvingUtilizationOfUnsecuredLines'] <= self.RevolvingUtilizationOfUnsecuredLines_drop_threshold) &\n",
    "            (X['age'] >= self.age_low_drop_threshold) & \n",
    "            (X['age'] <= self.age_up_drop_threshold) &\n",
    "            (X['NumberRealEstateLoansOrLines']<=self.NumberRealEstateLoansOrLines_drop_threshold)   \n",
    "               )\n",
    "\n",
    "        X_clean = X[mask].copy()\n",
    "\n",
    "        if y is not None:\n",
    "            y_clean = y[mask].copy()\n",
    "            return X_clean, y_clean\n",
    "            \n",
    "        return X_clean"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7b49e9ee-4c53-4c35-91bf-9f9f1b78df92",
   "metadata": {},
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler, MinMaxScaler, MaxAbsScaler\n",
    "\n",
    "class CreditScaler(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Масштабирует только не-булевые колонки.\n",
    "    Можно задать различные способы масштабирования\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, scaler_type='standard'):\n",
    "        \"\"\"\n",
    "        Параметр scaler_type - тип scaler'а.\n",
    "        \n",
    "        Доступные типы:\n",
    "        - 'standard': StandardScaler (среднее=0, дисперсия=1)\n",
    "        - 'robust': RobustScaler (устойчив к выбросам)\n",
    "        - 'minmax': MinMaxScaler (приводит к [0, 1])\n",
    "        - 'maxabs': MaxAbsScaler (приводит к [-1, 1])\n",
    "        \"\"\"\n",
    "\n",
    "        self.boolean_columns = [\n",
    "            'RevolvingUtilizationOverOne',\n",
    "            'DebtPayments_over_10k',\n",
    "            'Code96',\n",
    "            'Code98',\n",
    "            'NumberRealEstateLoansOrLines_over_5',\n",
    "            'Consumer_1_loans',\n",
    "            'Consumer_2-5_loans',\n",
    "            'Consumer_16-30_loans',\n",
    "            'RealEstateLoans_1-3_loans',\n",
    "            'RealEstateLoans_4+_loans'\n",
    "        ]\n",
    "        \n",
    "        self.scaler_type = scaler_type\n",
    "        self._create_scaler()\n",
    "        \n",
    "        # Эти переменные заполнятся во время fit\n",
    "        self.columns_to_scale_ = None\n",
    "        self.n_features_in_ = None\n",
    "        self.feature_names_in_ = None\n",
    "    \n",
    "    def _create_scaler(self):\n",
    "        \"\"\"Создает scaler по типу\"\"\"\n",
    "        if self.scaler_type == 'standard':\n",
    "            self.scaler = StandardScaler()\n",
    "        elif self.scaler_type == 'robust':\n",
    "            self.scaler = RobustScaler()\n",
    "        elif self.scaler_type == 'minmax':\n",
    "            self.scaler = MinMaxScaler()\n",
    "        elif self.scaler_type == 'maxabs':\n",
    "            self.scaler = MaxAbsScaler()\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                f\"Unknown scaler_type: {self.scaler_type}. \"\n",
    "                f\"Available: standard, robust, minmax, maxabs\"\n",
    "            )\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        \"\"\"\n",
    "        Определяет колонки для масштабирования (все, кроме булевых)\n",
    "        и обучает scaler.\n",
    "        \"\"\"\n",
    "        \n",
    "        self.feature_names_in_ = X.columns.tolist()\n",
    "        self.n_features_in_ = len(self.feature_names_in_)\n",
    "        \n",
    "        self.columns_to_scale_ = [\n",
    "            col for col in self.feature_names_in_ \n",
    "            if col not in self.boolean_columns\n",
    "        ]\n",
    "        \n",
    "        self.scaler.fit(X[self.columns_to_scale_])\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        \"\"\"\n",
    "        Масштабирует только не-булевы колонки.\n",
    "        \"\"\"\n",
    "        X_copy = X.copy()\n",
    "        \n",
    "        X_copy[self.columns_to_scale_] = self.scaler.transform(X_copy[self.columns_to_scale_])\n",
    "        \n",
    "        return X_copy\n",
    "    \n",
    "    def fit_transform(self, X, y=None):\n",
    "        return self.fit(X, y).transform(X, y)\n",
    "    \n",
    "    def get_feature_names_out(self, input_features=None):\n",
    "        \"\"\"Для совместимости с sklearn\"\"\"\n",
    "        if input_features is not None:\n",
    "            return input_features\n",
    "        return self.feature_names_in_ if self.feature_names_in_ is not None else []\n",
    "    \n",
    "    def set_params(self, **params):\n",
    "        \"\"\"Для совместимости с GridSearchCV\"\"\"\n",
    "        if 'scaler_type' in params and params['scaler_type'] != self.scaler_type:\n",
    "            self.scaler_type = params['scaler_type']\n",
    "            self._create_scaler()\n",
    "        return super().set_params(**params)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ba4dc8eb-cb39-45c7-ab48-81f18a20590b",
   "metadata": {},
   "source": [
    "train_label = train_data['SeriousDlqin2yrs']\n",
    "train_data = train_data.drop(columns = ['SeriousDlqin2yrs']).copy()\n",
    "\n",
    "credit_transformer = CreditDataPreprocessor()\n",
    "\n",
    "train_data_cleaned, train_label_cleaned = credit_transformer.clean_train(train_data, train_label)\n",
    "\n",
    "credit_transformer.fit(train_data_cleaned)\n",
    "train_data_transformed = credit_transformer.transform(train_data_cleaned)\n",
    "test_data_transformed = credit_transformer.transform(test_data)\n",
    "\n",
    "credit_scaler_standard = CreditScaler(scaler_type='standard').fit(train_data_transformed)\n",
    "\n",
    "train_data_scaled = credit_scaler_standard.transform(train_data_transformed)\n",
    "test_data_scaled = credit_scaler_standard.transform(test_data_transformed)\n",
    "\n",
    "import joblib\n",
    "joblib.dump(credit_transformer, FOLDER_PATH + 'processing/credit_transformer_120.pkl')\n",
    "joblib.dump(credit_scaler_standard, FOLDER_PATH + 'processing/credit_scaler_standard_120.pkl')\n",
    "\n",
    "train_data_scaled.to_csv(FOLDER_PATH + 'datasets/train_data_120.csv')\n",
    "train_label_cleaned.to_csv(FOLDER_PATH + 'datasets/train_label_120.csv')\n",
    "test_data_scaled.to_csv(FOLDER_PATH + 'datasets/test_data_120.csv')"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b24836c1-e276-4c2b-a25a-45adb5d5efb8",
   "metadata": {},
   "source": [
    "train_label_full = train_data_full['SeriousDlqin2yrs']\n",
    "train_full = train_data_full.drop(columns = ['SeriousDlqin2yrs']).copy()\n",
    "\n",
    "credit_transformer_full = CreditDataPreprocessor()\n",
    "train_full_cleaned, train_label_full_cleaned = credit_transformer_full.clean_train(train_full, train_label_full)\n",
    "train_full_transformed = credit_transformer_full.fit_transform(train_full_cleaned)\n",
    "\n",
    "credit_scaler_standard_full = CreditScaler(scaler_type='standard')\n",
    "train_full_scaled = credit_scaler_standard_full.fit_transform(train_full_transformed)\n",
    "\n",
    "import joblib\n",
    "joblib.dump(credit_transformer_full, FOLDER_PATH + 'processing/credit_transformer_150.pkl')\n",
    "joblib.dump(credit_scaler_standard_full, FOLDER_PATH + 'processing/credit_scaler_standard_150.pkl')\n",
    "\n",
    "train_full_scaled.to_csv(FOLDER_PATH + 'datasets/train_data_150.csv')\n",
    "train_label_full_cleaned.to_csv(FOLDER_PATH + 'datasets/train_label_150.csv')"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "acc25d9b-4fd2-421d-9b10-70567162fa77",
   "metadata": {},
   "source": [
    "# train_transformer = CreditDataPreprocessor(remove_outliers = False).fit(train_data)\n",
    "# train_data_transformed = train_transformer.transform(train_data)\n",
    "# test_data_transformed = train_transformer.transform(test_data)\n",
    "\n",
    "# joblib.dump(train_transformer, FOLDER_PATH + 'models/credit_preprocessor.pkl')\n",
    "\n",
    "# y_train = train_data_transformed['SeriousDlqin2yrs']\n",
    "# y_val = test_data_transformed['SeriousDlqin2yrs']\n",
    "\n",
    "# X_train = train_data_transformed.drop(columns = ['SeriousDlqin2yrs'])\n",
    "# X_val = test_data_transformed.drop(columns = ['SeriousDlqin2yrs'])\n",
    "\n",
    "# X_train_scaled_df, X_val_scaled_df = scaling_function(\n",
    "#     X_train, \n",
    "#     X_val)\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "934896e8-1ba2-46a3-9a57-8be5afe3417d",
   "metadata": {},
   "source": [
    "# full_transformed = CreditDataPreprocessor(remove_outliers = False).fit(train_data_full)\n",
    "# train_full_transformed = full_transformed.transform(train_data_full)\n",
    "# Kaggle_test_full_transformed = full_transformed.transform(test_data_full)\n",
    "\n",
    "# full_transformed = CreditDataPreprocessor(remove_outliers = True).fit(train_data_full)\n",
    "# train_full_transformed = full_transformed.transform(train_data_full)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0c4964af-014d-4564-bba5-79d3e8517aec",
   "metadata": {},
   "source": [
    "# from sklearn.linear_model import LogisticRegression\n",
    "# from xgboost import XGBClassifier\n",
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "# from sklearn.metrics import roc_auc_score\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# from sklearn.inspection import permutation_importance\n",
    "\n",
    "\n",
    "\n",
    "# y_train = train_data_transformed['SeriousDlqin2yrs']\n",
    "# y_val = test_data_transformed['SeriousDlqin2yrs']\n",
    "\n",
    "# X_train = train_data_transformed.drop(columns = ['SeriousDlqin2yrs'])\n",
    "# X_val = test_data_transformed.drop(columns = ['SeriousDlqin2yrs'])\n",
    "\n",
    "# X_train_scaled_df, X_val_scaled_df = scaling_function(\n",
    "#     X_train, \n",
    "#     X_val)\n",
    "\n",
    "\n",
    "# scale_pos_weight = len(y_train[y_train == 0]) / len(y_train[y_train == 1])\n",
    "# results = {}\n",
    "\n",
    "# models = {\n",
    "#     'LogisticRegression': LogisticRegression(\n",
    "#         max_iter=1000,\n",
    "#         class_weight = 'balanced',\n",
    "#         random_state = 42   \n",
    "#     ),\n",
    "    \n",
    "#     'RandomForest': RandomForestClassifier(\n",
    "#         n_estimators=100, \n",
    "#         max_depth=5,\n",
    "#         class_weight = 'balanced',\n",
    "#         random_state = 42\n",
    "#     ),\n",
    "    \n",
    "#     'XGBoost': XGBClassifier(\n",
    "#         n_estimators=100, \n",
    "#         max_depth=3,\n",
    "#         scale_pos_weight = scale_pos_weight,\n",
    "#         eval_metric = 'logloss',\n",
    "#         random_state = 42\n",
    "#     )\n",
    "# }\n",
    "\n",
    "\n",
    "# print(\"ROC-AUC:\")\n",
    "# for model_name, model in models.items():\n",
    "#     if model_name == 'LogisticRegression':\n",
    "#         X_train_use = X_train_scaled_df\n",
    "#         X_val_use = X_val_scaled_df\n",
    "        \n",
    "#     else:\n",
    "#         X_train_use = X_train\n",
    "#         X_val_use = X_val\n",
    "        \n",
    "#     model.fit(X_train_use, y_train)\n",
    "#     y_pred = model.predict_proba(X_val_use)[:,1]\n",
    "#     roc_auc = roc_auc_score(y_val, y_pred)\n",
    "#     results[model_name] = roc_auc\n",
    "#     print(f'{model_name}: {roc_auc:.4f}')\n",
    "\n",
    "#  # Permutation Importance\n",
    "\n",
    "# importance_xgb = XGBClassifier(\n",
    "#         n_estimators=100, \n",
    "#         max_depth=3,\n",
    "#         scale_pos_weight = scale_pos_weight,\n",
    "#         eval_metric = 'logloss',\n",
    "#         random_state = 42)\n",
    "\n",
    "# importance_xgb.fit(X_train, y_train)\n",
    "    \n",
    "\n",
    "# perm_result = permutation_importance(\n",
    "#     importance_xgb,\n",
    "#     X_val, y_val,\n",
    "#     n_repeats = 10,\n",
    "#     scoring = 'roc_auc',\n",
    "#     random_state = 42)\n",
    "\n",
    "# perm_importance_df = pd.DataFrame({\n",
    "#     'feature': X_val.columns,\n",
    "#     'perm_importance': perm_result.importances_mean,\n",
    "#     'perm_std': perm_result.importances_std\n",
    "# }).sort_values('perm_importance', ascending = False)\n",
    "\n",
    "# print('Permutation Importance')\n",
    "# print(perm_importance_df)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1de12403-2918-45a4-a426-cf38b0aa3563",
   "metadata": {},
   "source": [
    "# train_label = train_full_transformed['SeriousDlqin2yrs']\n",
    "\n",
    "# train_full_scaled, Kaggle_test_full_scaled = scaling_function(\n",
    "#     train_full_transformed.drop(columns = 'SeriousDlqin2yrs'), \n",
    "#     Kaggle_test_full_transformed)\n",
    "\n",
    "# train_full_scaled.to_csv(FOLDER_PATH + 'data/train_full_scaled_pipeline_v1.csv')\n",
    "# Kaggle_test_full_scaled.to_csv(FOLDER_PATH + 'data/Kaggle_test_full_scaled_pipeline_v1.csv')\n",
    "# train_label.to_csv(FOLDER_PATH + '/data/train_label_pipeline_v1.csv')"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4a225cc-4e3e-4641-b747-f34e6592b64e",
   "metadata": {},
   "source": [],
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
