{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2f9b8b2-6b9e-4311-a45a-1295c7d91ed4",
   "metadata": {},
   "source": [
    "train_data['NumberOfDependents'] = train_data['NumberOfDependents'].fillna(value = 0)\n",
    "train_data['NumberOfDependents'] = train_data['NumberOfDependents'].clip(0,10).copy()\n",
    "\n",
    "train_data['MonthlyIncomeIsMissing'] = 0\n",
    "train_data.loc[train_data['MonthlyIncome'].isna(), 'MonthlyIncomeIsMissing'] = 1\n",
    "train_data['MonthlyIncome'] = train_data['MonthlyIncome'].fillna(value = 0)\n",
    "\n",
    "train_data = train_data[train_data['RevolvingUtilizationOfUnsecuredLines'] <= 2].copy()\n",
    "train_data['RevolvingUtilizationOverOne'] = 0.0\n",
    "train_data.loc[train_data['RevolvingUtilizationOfUnsecuredLines'] > 1, 'RevolvingUtilizationOverOne'] = 1.0\n",
    "train_data['RevolvingUtilizationOfUnsecuredLines'] = train_data['RevolvingUtilizationOfUnsecuredLines'].clip(0,1).copy()\n",
    "\n",
    "train_data = train_data[train_data['age'] >= 18].copy()\n",
    "\n",
    "train_data = train_data[train_data['age'] <= 80].copy()\n",
    "\n",
    "train_data['DebtPayments'] = 0.0\n",
    "train_data.loc[train_data['MonthlyIncome'] == 0,'DebtPayments'] = train_data.loc[train_data['MonthlyIncome'] == 0,'DebtRatio']\n",
    "train_data.loc[train_data['MonthlyIncome'] != 0,'DebtPayments'] = train_data.loc[train_data['MonthlyIncome'] != 0,'DebtRatio'] * train_data.loc[train_data['MonthlyIncome'] != 0,'MonthlyIncome']\n",
    "train_data['DebtRatio'] = train_data['DebtRatio'].clip(0,5).copy()\n",
    "\n",
    "\n",
    "train_data['DebtPayments_over_10k'] = 0.0\n",
    "train_data.loc[train_data['DebtPayments'] > 10000,'DebtPayments_over_10k'] = 1.0\n",
    "train_data['DebtPayments'] = train_data['DebtPayments'].clip(0,10000).copy()\n",
    "\n",
    "train_data['MonthlyIncome_over_20k'] = 0.0\n",
    "train_data.loc[train_data['MonthlyIncome'] >= 20e3,'MonthlyIncome_over_20k'] = 1.0\n",
    "train_data['MonthlyIncome'] = train_data['MonthlyIncome'].clip(0,20e3)\n",
    "\n",
    "train_data['Code96'] = 0.0\n",
    "train_data['Code98'] = 0.0\n",
    "train_data.loc[train_data['NumberOfTime30-59DaysPastDueNotWorse'] == 96, 'Code96']  = 1.0\n",
    "train_data.loc[train_data['NumberOfTime30-59DaysPastDueNotWorse'] == 98, 'Code98']  = 1.0\n",
    "\n",
    "PastDueRiskScore_weights = [1.0, 1.2, 1.3]\n",
    "train_data['PastDueRiskScore'] = (\n",
    "    PastDueRiskScore_weights[0] * train_data['NumberOfTime30-59DaysPastDueNotWorse'] +\n",
    "    PastDueRiskScore_weights[1] * train_data['NumberOfTime60-89DaysPastDueNotWorse'] +\n",
    "    PastDueRiskScore_weights[2] * train_data['NumberOfTimes90DaysLate'])\n",
    "train_data.loc[train_data['NumberOfTime30-59DaysPastDueNotWorse'] == 96, 'PastDueRiskScore'] = 96\n",
    "train_data.loc[train_data['NumberOfTime30-59DaysPastDueNotWorse'] == 98, 'PastDueRiskScore'] = 98\n",
    "train_data = train_data.drop(columns = ['NumberOfTime30-59DaysPastDueNotWorse', 'NumberOfTime60-89DaysPastDueNotWorse', 'NumberOfTimes90DaysLate'])\n",
    "\n",
    "train_data['NumberOfOpenCreditLinesAndLoans_over_30'] = 0.0\n",
    "train_data.loc[train_data['NumberOfOpenCreditLinesAndLoans'] > 30, 'NumberOfOpenCreditLinesAndLoans_over_30'] = 1.0\n",
    "train_data['NumberOfOpenCreditLinesAndLoans'] = train_data['NumberOfOpenCreditLinesAndLoans'].clip(0,30).copy()\n",
    "\n",
    "train_data = train_data[train_data['NumberRealEstateLoansOrLines']<=20].copy()\n",
    "train_data['NumberRealEstateLoansOrLines_over_5'] = 0.0\n",
    "train_data.loc[train_data['NumberRealEstateLoansOrLines'] > 5, 'NumberRealEstateLoansOrLines_over_5'] = 1.0\n",
    "train_data['NumberRealEstateLoansOrLines'] = train_data['NumberRealEstateLoansOrLines'].clip(0,5).copy()\n",
    "\n",
    "train_data['ConsumerCredit_Group'] = pd.cut(train_data['NumberOfOpenCreditLinesAndLoans'], \n",
    "                                bins = [0,1, 2,6,15,31], \n",
    "                                labels=[\n",
    "                                    '0_loans',\n",
    "                                    '1_loans',\n",
    "                                    '2-5_loans',\n",
    "                                    '6-14_loans',\n",
    "                                    '16-30_loans'\n",
    "                                ])\n",
    "consumer_dummy = pd.get_dummies(train_data['ConsumerCredit_Group'], prefix='Consumer', drop_first = False).astype('float')\n",
    "\n",
    "train_data['RealEstateLoans_Group'] = pd.cut(train_data['NumberRealEstateLoansOrLines'],\n",
    "                                   bins=[-1, 0, 3,100], \n",
    "                                   labels= [\n",
    "                                            '0_loans',      \n",
    "                                            '1-3_loans',    \n",
    "                                            '4+_loans',    \n",
    "                                            ])\n",
    "estate_dummy = pd.get_dummies(train_data['RealEstateLoans_Group'], prefix='RealEstateLoans', drop_first = False).astype('float')\n",
    "\n",
    "train_data = pd.concat([train_data, consumer_dummy, estate_dummy], axis = 1).copy()\n",
    "train_data = train_data.drop(columns = ['ConsumerCredit_Group', \n",
    "                                        'RealEstateLoans_Group']).copy()\n",
    "\n",
    "train_data = train_data.drop(columns = ['Consumer_6-14_loans',  \n",
    "                                        'RealEstateLoans_0_loans']).copy()\n",
    "\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b672a211-d852-4b98-b203-6c8134eeca73",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "import plotly.express as px"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0397e60a-a61d-46dd-93d4-b850ab781a7d",
   "metadata": {},
   "source": [
    "FOLDER_PATH = '/Users/artemzmailov/Desktop/GiveMeSomeCredit/'\n",
    "train_data_full = pd.read_csv(FOLDER_PATH + 'data/cs-training.csv', index_col = 0)\n",
    "train_label_full = train_data_full['SeriousDlqin2yrs']\n",
    "#test_data_full = pd.read_csv(FOLDER_PATH + 'data/cs-test.csv', index_col = 0)\n",
    "#test_data_full = test_data_full.drop(columns = ['SeriousDlqin2yrs'])\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3c3996fb-7edf-4007-ba6b-f26d650fcce1",
   "metadata": {},
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_data, test_data, train_label, test_label = train_test_split(\n",
    "    train_data_full,\n",
    "    train_label_full,\n",
    "    test_size = 0.2, \n",
    "    stratify = train_label_full,\n",
    "    shuffle = True,\n",
    "    random_state = 42)\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "bb07b9eb-a4d4-47ed-88ca-d9d90636cd48",
   "metadata": {},
   "source": [
    "## Domain EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0eef0648-a1ca-4ed8-b5e4-a9ea23031afb",
   "metadata": {},
   "source": [
    "'''\n",
    "Анализ всех данных на предмет логических закономерностей, нереалистичных значений исходя из логики предметной области\n",
    "\n",
    "SeriousDlqin2yrs - Person experienced 90 days past due delinquency or worse (Y/N)\n",
    "\n",
    "RevolvingUtilizationOfUnsecuredLines - Total balance on credit cards and personal lines of credit \n",
    "except real estate and no installment debt like car loans divided by the sum of credit limits (percentage)\n",
    "\n",
    "age\t- Age of borrower in years (integer)\n",
    "\n",
    "NumberOfTime30-59DaysPastDueNotWorse - Number of times borrower has been 30-59 days past due but no worse in the last 2 years. (integer)\n",
    "\n",
    "DebtRatio - Monthly debt payments, alimony,living costs divided by monthy gross income (percentage)\n",
    "\n",
    "MonthlyIncome - Monthly income (real)\n",
    "\n",
    "NumberOfOpenCreditLinesAndLoans\t- Number of Open loans (installment like car loan or mortgage) \n",
    "and Lines of credit (e.g. credit cards) (integer)\n",
    "\n",
    "NumberOfTimes90DaysLate\t- Number of times borrower has been 90 days or more past due. (integer)\n",
    "\n",
    "NumberRealEstateLoansOrLines - Number of mortgage and real estate loans including home equity lines of credit (integer)\n",
    "\n",
    "NumberOfTime60-89DaysPastDueNotWorse - Number of times borrower has been 60-89 days past due but no worse in the last 2 years. (integer)\n",
    "\n",
    "NumberOfDependents - Number of dependents in family excluding themselves (spouse, children etc.) (integer)\n",
    "'''"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cd5ea7e7-63ee-4df4-92c0-592336e2fd4e",
   "metadata": {},
   "source": [
    "def hist_print(data, graph):\n",
    "    for col in data.columns:\n",
    "        if col != 'SeriousDlqin2yrs':\n",
    "            fig = graph(data[col])\n",
    "            print(f'col: {col}, min: {data[col].min()}, max: {data[col].max()}')\n",
    "            fig.show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ff620269-5d8f-4c14-858c-0bd8056ba5ed",
   "metadata": {},
   "source": [
    "# hist_print(train_data, px.histogram)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fd8b340a-6953-4fcb-ae8e-129ff573a4de",
   "metadata": {},
   "source": [
    "'''\n",
    "Полезные инсайты из всех данных:\n",
    "RevolvingUtilizationOfUnsecuredLines - имеет запредельно высокий максимум, вероятно аномалия\n",
    "\n",
    "age - возраст более 80 и даже 100 лет, в большинстве стран мира кредиты дают до 70-80 лет, \n",
    "всё что выше вероятно стоит либо отдать на ручной разбор (кредит под специальные условия) или автоотказ\n",
    "\n",
    "NumberOfTime30-59DaysPastDueNotWorse - странные значения 96 и 98, \n",
    "\n",
    "DebtRatio - нереалистичный максимум = 329664, а также нереалистичный правый хвост, \n",
    "вряд ли отношение платежей к доходу может быть более чем в 100 раз больше\n",
    "\n",
    "MonthlyIncome - есть люди с очень высоким доходом\n",
    "NumberOfOpenCreditLinesAndLoans - длинный правый хвост, но +- непрерывный\n",
    "NumberOfTimes90DaysLate - снова значения 96 и 98\n",
    "NumberRealEstateLoansOrLines - длинный правый хвост, но +- непрерывный\n",
    "NumberOfTime60-89DaysPastDueNotWorse - снова значения 96 и 98\n",
    "NumberOfDependents - странный максимум в 20, в семье заёмщика 20 человек?\n",
    "\n",
    "В NumberOfTime30-59DaysPastDueNotWorse, NumberOfTime60-89DaysPastDueNotWorse, NumberOfTimes90DaysLate повторяются значения 96, 98\n",
    "\n",
    "'''"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "597af9a4-6305-4aa3-a9ef-a407f9b823f3",
   "metadata": {},
   "source": [
    "train_data.drop(columns = ['NumberOfDependents']).corrwith(train_data['NumberOfDependents']).sort_values(ascending = False)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ad83db97-89a9-4945-902c-34b41b5ce1bd",
   "metadata": {},
   "source": [
    "'''\n",
    "Доля положительного таргета ниже чем у 0-вого значения, \n",
    "с ростом числа иждивенцев доля положительного таргета только растёт, потому заполняем пропуски нулями.\n",
    "'''\n",
    "train_data['NumberOfDependents'] = train_data['NumberOfDependents'].fillna(value = 0)\n",
    "# больше 10 иждивенцев - очень редкий случай, 10 - как понятное круглое значение для обозначения очень многодетной семьи\n",
    "train_data['NumberOfDependents'] = train_data['NumberOfDependents'].clip(0,10).copy()\n",
    "#test_data['NumberOfDependents'] = test_data['NumberOfDependents'].fillna(value = 0)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "34a77f6b-732a-43bf-9cb8-a11fcf8ae91e",
   "metadata": {},
   "source": [
    "### Пропуски MonthlyIncome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "69c106f3-1fbc-4e1a-91b9-74902bbb9be4",
   "metadata": {},
   "source": [
    "'''\n",
    "Простая бизнес-логика, если по какой-то причине не указан доход, странно рисовать человеку какой-то доход, \n",
    "который может быть больше реального и тем самым повышать шанс на одобрение\n",
    "при этом число людей с неуказанным доходом велико, а число людей с нулевым доходом всего 1322,\n",
    "потому выполняется заполнение пропуском нулем и создание признака-флага для лиц с неуказанным доходом\n",
    "'''\n",
    "train_data['MonthlyIncomeIsMissing'] = 0\n",
    "train_data.loc[train_data['MonthlyIncome'].isna(), 'MonthlyIncomeIsMissing'] = 1\n",
    "train_data['MonthlyIncome'] = train_data['MonthlyIncome'].fillna(value = 0)\n",
    "test_data['MonthlyIncome'] = test_data['MonthlyIncome'].fillna(value = 0)\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "337b7a95-115d-4267-9e30-d7a71c1b8a18",
   "metadata": {},
   "source": [
    "'''\n",
    "Технически соотношение выше 1 возможно, если допустим овердрафт по карте, \n",
    "но обычно на более плохих условиях (выше коммисия или %), и шанс получить одобрение нового займа куда ниже,\n",
    "Статистика также показывает резкий рост доли лиц с просрочкой при прохождении порога = 1 \n",
    "их доля возрастает с 0.19 до 0.35 и в последствии держится на уровнях 0.4-0.5\n",
    "Примеров с соотношением выше 2х мало и их оценки статистически не обоснованы, тк в реальной жизни такие случаи - большая редкость, \n",
    "не говоря уже о соотношении выше 10, что в общем-то невозможно, а если и возможно, то такие случаи уходят на ручной разбор \n",
    "потому всё что выше 2х - дропается\n",
    "'''\n",
    "train_data = train_data[train_data['RevolvingUtilizationOfUnsecuredLines'] <= 2].copy()\n",
    "train_data['RevolvingUtilizationOverOne'] = 0.0\n",
    "\n",
    "train_data.loc[train_data['RevolvingUtilizationOfUnsecuredLines'] > 1, 'RevolvingUtilizationOverOne'] = 1.0"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "8cb24015-229f-480b-bb89-1d2e3554a954",
   "metadata": {},
   "source": [
    "## Outliers Age "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "180dbc9d-1399-4bba-afbf-400930e28ca5",
   "metadata": {},
   "source": [
    "train_data = train_data[train_data['age'] >= 18].copy()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "39a0903d-976b-4906-8307-ed2f3b1cca26",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "'''\n",
    "кредиты старше 80 выдают редко и на особых условиях, \n",
    "так что такие случаи автоматически уходят на ручной разбор или отказ\n",
    "'''\n",
    "train_data = train_data[train_data['age'] <= 80].copy()\n",
    "#test_data = test_data[test_data['age'] <= 80].copy()"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "bd8587b0-0bfb-45e6-a30a-1286e6816c5b",
   "metadata": {},
   "source": [
    "## Outliers DebtRatio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "fea2417c-112c-4261-8c16-6f86def8318c",
   "metadata": {},
   "source": [
    "train_data['DebtPayments'] = 0.0\n",
    "train_data.loc[train_data['MonthlyIncome'] == 0,'DebtPayments'] = train_data.loc[train_data['MonthlyIncome'] == 0,'DebtRatio']\n",
    "\n",
    "train_data.loc[train_data['MonthlyIncome'] != 0,'DebtPayments'] = train_data.loc[train_data['MonthlyIncome'] != 0,'DebtRatio'] * train_data.loc[train_data['MonthlyIncome'] != 0,'MonthlyIncome']\n",
    "\n",
    "train_data['DebtRatio'] = train_data['DebtRatio'].clip(0,5).copy()\n",
    "# test_data['DebtPayments'] = 0\n",
    "# test_data.loc[test_data['MonthlyIncome'] == 0,'DebtPayments'] = test_data.loc[test_data['MonthlyIncome'] == 0,'DebtRatio']\n",
    "\n",
    "# test_data.loc[test_data['MonthlyIncome'] != 0,'DebtPayments'] = test_data.loc[test_data['MonthlyIncome'] != 0,'DebtRatio'] * test_data.loc[test_data['MonthlyIncome'] != 0,'MonthlyIncome']\n",
    "\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "a79c3bfe-3302-45e0-a1d5-aec31b749be7",
   "metadata": {},
   "source": [
    "'''\n",
    "после 10к значений в бинах слишком мало, доля положительного таргета растёт до порога признака в 10к, далее стабилизируется +- около 0.15, \n",
    "потому делаем клип и признак-флаг - платежи от 10 тыс долларов. Понятно интерпретируемый признак.\n",
    "'''\n",
    "\n",
    "train_data['DebtPayments_over_10k'] = 0.0\n",
    "train_data.loc[train_data['DebtPayments'] > 10000,'DebtPayments_over_10k'] = 1.0\n",
    "train_data['DebtPayments'] = train_data['DebtPayments'].clip(0,10000).copy()"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "f3091880-23d3-4c83-bf97-67d6013ce6da",
   "metadata": {},
   "source": [
    "## Outliers MonthlyIncome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "ae030f32-c7f5-4a1d-be68-979ae1d57600",
   "metadata": {},
   "source": [
    "'''\n",
    "больше 20 тыс MonthlyIncome - 1821 пример, iqr граница - ~16200, \n",
    "однако с ростом дохода доля положительного таргета не сильно меняется и остаётся на уровне 0.04-0.05, \n",
    "потому делается клип + признак-флаг, что доход больше 20 тыс долларов.\n",
    "дропа нет, тк доходы выше 20 тыс долларов вполне реальны, однако риск дефолта для них остаётся на уровне от 20 тыс +-\n",
    "'''\n",
    "train_data['MonthlyIncome_over_20k'] = 0.0\n",
    "train_data.loc[train_data['MonthlyIncome'] >= 20e3,'MonthlyIncome_over_20k'] = 1.0\n",
    "train_data['MonthlyIncome'] = train_data['MonthlyIncome'].clip(0,20e3)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "19ed55b0-e019-40ef-b525-0e59e839f8c7",
   "metadata": {},
   "source": [
    "## Outliers PastDueNotWorse\n",
    "NumberOfTime30-59DaysPastDueNotWorse, \n",
    "NumberOfTime60-89DaysPastDueNotWorse, \n",
    "NumberOfTimes90DaysLate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "3eec57d3-90d9-44cb-8152-9967efd25854",
   "metadata": {},
   "source": [
    "'''\n",
    "Необходимо посмотреть на значения 96, 98.\n",
    "Также три признака являюстя линейно зависимыми, потому после обработки выбросов стоит создать новый признак, как их линейную комбинацию\n",
    "'''"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "3b54046a-5788-4189-8d44-618f3f4e9c96",
   "metadata": {},
   "source": [
    "'''\n",
    "Значения 96 и 98 у трёх признаков: \n",
    "NumberOfTime30-59DaysPastDueNotWorse, NumberOfTime60-89DaysPastDueNotWorse, NumberOfTimes90DaysLate - специальные коды, \n",
    "доля случаев просрочки при их наличии колоссальна: 1.0 и ~0.54\n",
    "\n",
    "Вероятно: Вот что коды 96 и 98 означают в типичной кодировке российских бюро кредитных историй (например, НБКИ):\n",
    "Код 96: Безнадежный долг / Передан на взыскание. \n",
    "Этот статус ставится, когда банк признает задолженность невозможной к взысканию стандартными методами. \n",
    "Часто это означает, что просрочка превышает 180–360 дней. \n",
    "Кредит может быть передан коллекторам по договору агентского обслуживания, но все еще числится на балансе банка.\n",
    "Код 98: Списание долга / Продажа (Уступка прав). \n",
    "Кредит закрыт в системе банка по причине списания за счет резервов или продажи долга сторонней организации (коллекторскому агентству) по договору цессии. \n",
    "Для заемщика это худший статус: долг не исчезает, но теперь его взыскивает новый владелец, а кредитная история считается максимально испорченной.\n",
    "\n",
    "Потому для прода случаи с такими кодами идут автоматически на ручной разбор или автоотказ, \n",
    "так что для прод модели они дропаются, но для kaggle модели для максимального скора будут созданы фичи-флаги на эти коды\n",
    "'''\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "18b2b959-928d-4994-91d5-e91fcad72f4f",
   "metadata": {},
   "source": [
    "print(train_data['NumberOfTime30-59DaysPastDueNotWorse'].unique())\n",
    "print(train_data['NumberOfTime60-89DaysPastDueNotWorse'].unique())\n",
    "print(train_data['NumberOfTimes90DaysLate'].unique())"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "b3e61c69-5c4c-47a8-8517-4a2f7b843583",
   "metadata": {},
   "source": [
    "# Обработка для Kaggle\n",
    "train_data['Code96'] = 0.0\n",
    "train_data['Code98'] = 0.0\n",
    "\n",
    "train_data.loc[train_data['NumberOfTime30-59DaysPastDueNotWorse'] == 96, 'Code96']  = 1.0\n",
    "train_data.loc[train_data['NumberOfTime30-59DaysPastDueNotWorse'] == 98, 'Code98']  = 1.0\n",
    "\n",
    "#code_replacement = 30\n",
    "# cols_for_replacement = ['NumberOfTime30-59DaysPastDueNotWorse', 'NumberOfTime60-89DaysPastDueNotWorse', 'NumberOfTimes90DaysLate']\n",
    "# train_data.loc[train_data['NumberOfTime30-59DaysPastDueNotWorse'] == 96, cols_for_replacement] = code_replacement\n",
    "# train_data.loc[train_data['NumberOfTime30-59DaysPastDueNotWorse'] == 98, cols_for_replacement] = code_replacement\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "9cf2d7e5-a49e-4f8c-8cfb-abcfd03a802a",
   "metadata": {},
   "source": [
    "# def unique_values_pos_neg_analysis(data, col):\n",
    "#     grps = data[[col, 'SeriousDlqin2yrs']].groupby(col).groups\n",
    "#     group_koef = 0\n",
    "#     koef_cnt = 0\n",
    "#     for idx, grp in grps.items():\n",
    "#         grp_len = len(grp)\n",
    "#         pos = sum(data['SeriousDlqin2yrs'].loc[grp])\n",
    "#         neg = grp_len - pos\n",
    "#         if grp_len > 1000 and pos != 0 and neg !=0:\n",
    "#             koef_cnt += 1\n",
    "#             group_koef += grp_len * pos/neg\n",
    "\n",
    "\n",
    "#             print(f'group: {idx}, postive: {pos}, negative: {neg}, grp_len: {grp_len}, pos/neg: {100*pos/neg if (pos!=0 and neg!=0) else 0:.2f}%')\n",
    "#     return group_koef/koef_cnt\n",
    "    \n",
    "# print('koef_30_59')\n",
    "# koef_30_59 = unique_values_pos_neg_analysis(train_data, 'NumberOfTime30-59DaysPastDueNotWorse')\n",
    "\n",
    "# print('koef_60_89')\n",
    "# koef_60_89 = unique_values_pos_neg_analysis(train_data, 'NumberOfTime60-89DaysPastDueNotWorse')\n",
    "\n",
    "# print('koef_90_')\n",
    "# koef_90_ = unique_values_pos_neg_analysis(train_data, 'NumberOfTimes90DaysLate')\n",
    "# print(koef_30_59, koef_60_89, koef_90_)\n",
    "\n",
    "# koefs_sum = koef_30_59 + koef_60_89 + koef_90_\n",
    "# koef_30_59_balanced = round(koef_30_59 / koefs_sum, 3)\n",
    "# koef_60_89_balanced = round(koef_60_89 / koefs_sum, 3)\n",
    "# koef_90_balanced = round(koef_90_ / koefs_sum, 3)\n",
    "# PastDueRiskScore_weights = [koef_30_59_balanced, koef_60_89_balanced, koef_90_balanced]\n",
    "# print(PastDueRiskScore_weights)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "31aae94d-22b6-46d4-b745-5088ff216b37",
   "metadata": {},
   "source": [
    "'''\n",
    "считаем долю положительного таргета для каждого значения признака, \n",
    "что встречается более 100 раз (для статистической значимости), \n",
    "суммируем и в конце делим на число просмотренных значений признака\n",
    "'''\n",
    "\n",
    "def unique_values_pos_rate_analysis(data, col):\n",
    "    grps = data[[col, 'SeriousDlqin2yrs']].groupby(col).groups\n",
    "    group_koef = 0\n",
    "    koef_cnt = 0\n",
    "    for idx, grp in grps.items():\n",
    "        grp_len = len(grp)\n",
    "        pos = data['SeriousDlqin2yrs'].loc[grp].mean()\n",
    "        neg = 1 - pos\n",
    "        if grp_len > 100:\n",
    "            koef_cnt += 1\n",
    "            group_koef += pos\n",
    "\n",
    "            print(f'group: {idx}, postive: {pos}, negative: {neg}, grp_len: {grp_len}')\n",
    "    return group_koef/koef_cnt\n",
    "    \n",
    "print('koef_30_59')\n",
    "koef_30_59 = unique_values_pos_rate_analysis(train_data, 'NumberOfTime30-59DaysPastDueNotWorse')\n",
    "\n",
    "print('koef_60_89')\n",
    "koef_60_89 = unique_values_pos_rate_analysis(train_data, 'NumberOfTime60-89DaysPastDueNotWorse')\n",
    "\n",
    "print('koef_90_')\n",
    "koef_90_ = unique_values_pos_rate_analysis(train_data, 'NumberOfTimes90DaysLate')\n",
    "print(koef_30_59, koef_60_89, koef_90_)\n",
    "\n",
    "# считаем веса относительно NumberOfTime30-59DaysPastDueNotWorse\n",
    "weights_relative = [\n",
    "    1.0,\n",
    "    koef_60_89 / koef_30_59,\n",
    "    koef_90_ / koef_30_59  \n",
    "]\n",
    "print(weights_relative)\n",
    "PastDueRiskScore_weights = [1.0, 1.2, 1.3]\n",
    "\n",
    "'''\n",
    "Округлили полученные значения, получились понятные коэффициенты:\n",
    "просрочка более 60 дней в 1.2 раза важнее просрочки до 60 дней, \n",
    "просрочка более 90 дней в 1.3 раза важнее просрочки до 60 дней, \n",
    "Такая комбинация логична (чем дольше просрочка, тем она критичнее), интерпретируема для регулятора. \n",
    "'''\n",
    "\n",
    "train_data['PastDueRiskScore'] = (\n",
    "    PastDueRiskScore_weights[0] * train_data['NumberOfTime30-59DaysPastDueNotWorse'] +\n",
    "    PastDueRiskScore_weights[1] * train_data['NumberOfTime60-89DaysPastDueNotWorse'] +\n",
    "    PastDueRiskScore_weights[2] * train_data['NumberOfTimes90DaysLate']\n",
    ")\n",
    "\n",
    "train_data.loc[train_data['NumberOfTime30-59DaysPastDueNotWorse'] == 96, 'PastDueRiskScore'] = 96\n",
    "train_data.loc[train_data['NumberOfTime30-59DaysPastDueNotWorse'] == 98, 'PastDueRiskScore'] = 98\n",
    "    \n",
    "train_data = train_data.drop(columns = ['NumberOfTime30-59DaysPastDueNotWorse', 'NumberOfTime60-89DaysPastDueNotWorse', 'NumberOfTimes90DaysLate'])\n",
    "\n",
    "# test_data['PastDueRiskScore'] = (\n",
    "#     PastDueRiskScore_weights[0] * test_data['NumberOfTime30-59DaysPastDueNotWorse'] +\n",
    "#     PastDueRiskScore_weights[1] * test_data['NumberOfTime60-89DaysPastDueNotWorse'] +\n",
    "#     PastDueRiskScore_weights[2] * test_data['NumberOfTimes90DaysLate']\n",
    "# )\n",
    "\n",
    "# test_data = test_data.drop(columns = ['NumberOfTime30-59DaysPastDueNotWorse', 'NumberOfTime60-89DaysPastDueNotWorse', 'NumberOfTimes90DaysLate'])\n",
    "\n",
    "\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "05f7ed46-7aba-46b0-8cd7-a9220fbd61ef",
   "metadata": {},
   "source": [
    "train_data.info()"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "96f11eb1-b05a-4c0e-b384-d0870f6c9ccd",
   "metadata": {},
   "source": [
    "### Outliers NumberOfOpenCreditLinesAndLoans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "5340b187-feca-405b-91ea-a67a479ee8b2",
   "metadata": {},
   "source": [
    "# outliers_check(train_data, 'NumberOfOpenCreditLinesAndLoans')"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "42716e90-c479-4e2c-990e-654ef6202513",
   "metadata": {},
   "source": [
    "train_data['NumberOfOpenCreditLinesAndLoans'].describe()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "54a0f642-7bb9-47c0-ac31-aaa9b84ea4ec",
   "metadata": {},
   "source": [
    "bins = [0,1,2,3,4,5,10,15,20,\n",
    "       25,30,60]\n",
    "interval_check(train_data, 'NumberOfOpenCreditLinesAndLoans', bins)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "6ead4fee-2b17-446b-804a-1811598f73a4",
   "metadata": {},
   "source": [
    "'''\n",
    "Делается клип до 30, после 30 доля положительного таргета начинает медленно расти, \n",
    "но данных уже очень мало для получения статистически обоснованных оценок, потому делается привычный клип до 30 + флаг,\n",
    "'''\n",
    "\n",
    "train_data['NumberOfOpenCreditLinesAndLoans_over_30'] = 0.0\n",
    "train_data.loc[train_data['NumberOfOpenCreditLinesAndLoans'] > 30, 'NumberOfOpenCreditLinesAndLoans_over_30'] = 1.0\n",
    "train_data['NumberOfOpenCreditLinesAndLoans'] = train_data['NumberOfOpenCreditLinesAndLoans'].clip(0,30)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "4aec1665-c13d-4a33-90f0-6499bae8db86",
   "metadata": {},
   "source": [
    "train_data.info()"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "7cbf51b8-c19a-47ab-9f4f-a313920ead3a",
   "metadata": {},
   "source": [
    "### Outliers NumberRealEstateLoansOrLines  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "a95debb2-9537-42e7-8f4f-1aa71b56221b",
   "metadata": {},
   "source": [
    "# outliers_check(train_data, 'NumberRealEstateLoansOrLines')"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "f49bf007-4b93-413d-bed0-2519e6efb8e5",
   "metadata": {},
   "source": [
    "train_data['NumberRealEstateLoansOrLines'].describe()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "a29cf2cb-14bc-49a2-af40-ff03b9ba38e2",
   "metadata": {},
   "source": [
    "bins = [0,1,2,3,4,5,\n",
    "        6,7,8,9,10,15,20,25,30,35,40,45,50,\n",
    "        55]\n",
    "interval_check(train_data, 'NumberRealEstateLoansOrLines', bins)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "616025b3-3f7e-43d0-a513-558a422f7577",
   "metadata": {},
   "source": [
    "train_data[(train_data['NumberRealEstateLoansOrLines']>=20)]"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "0d636e86-c115-4445-9d8e-463df68dda5e",
   "metadata": {},
   "source": [
    "'''\n",
    "более 5 ипотек имеют 1174 лица, доля положительного таргета (риск просрочки) у них заметно выше, но примеров мало, \n",
    "те риск скачет от бина к бину, однако остаётся повышенным. Потому делается клип до 5\n",
    "предварительно клип на 5 и дроп выше 20 ипотек, тк всего 11 примеров и похоже на шум в данных\n",
    "'''\n",
    "train_data = train_data[train_data['NumberRealEstateLoansOrLines']<=20].copy()\n",
    "train_data['NumberRealEstateLoansOrLines_over_5'] = 0.0\n",
    "train_data.loc[train_data['NumberRealEstateLoansOrLines'] > 5, 'NumberRealEstateLoansOrLines_over_5'] = 1.0\n",
    "\n",
    "train_data['NumberRealEstateLoansOrLines'] = train_data['NumberRealEstateLoansOrLines'].clip(0,5).copy()"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "d3474c9a-d471-4d5b-9cde-bf8db2f016e8",
   "metadata": {},
   "source": [
    "## Correlation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "d6f6130a-4354-41fd-b288-71d618945a15",
   "metadata": {},
   "source": [
    "train_data.info()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "a843dc54-a24c-41cf-9ad9-5ec77cad3ab9",
   "metadata": {},
   "source": [
    "train_data = train_data.astype('float')\n",
    "import seaborn as sns\n",
    "sns.heatmap(train_data.corr(), annot = True,annot_kws={'size': 6})"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "d63bd70f-df3e-4a58-b6d4-f7f54d1c87eb",
   "metadata": {},
   "source": [
    "def unique_values_pos_neg_analysis(data, col):\n",
    "    grps = pd.concat([data[col],train_label], axis = 1).groupby(col).groups\n",
    "    group_koef = 0\n",
    "    koef_cnt = 0\n",
    "    for idx, grp in grps.items():\n",
    "        grp_len = len(grp)\n",
    "        pos = sum(train_label.loc[grp])\n",
    "        neg = grp_len - pos\n",
    "        if grp_len > 1000 and pos != 0 and neg !=0:\n",
    "            koef_cnt += 1\n",
    "            group_koef += grp_len * pos/neg\n",
    "\n",
    "\n",
    "        print(f'group: {idx}, postive: {pos}, negative: {neg}, grp_len: {grp_len}, pos/neg: {100*pos/neg if (pos!=0 and neg!=0) else 0:.2f}%')\n",
    "    return group_koef/koef_cnt\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "eb25d9af-b9ad-4bd1-9ea0-f7133f95b0bd",
   "metadata": {},
   "source": [
    "unique_values_pos_neg_analysis(train_data,'NumberOfOpenCreditLinesAndLoans')"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "f03f7566-27ae-49a9-89b7-777f10283699",
   "metadata": {},
   "source": [
    "unique_values_pos_neg_analysis(train_data,'NumberRealEstateLoansOrLines')"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "dd87131a-8143-4dfe-9858-65f773a77ed9",
   "metadata": {},
   "source": [
    "train_data_no_label = train_data.drop(columns = ['DebtRatio', \n",
    "                                                 # 'NumberOfOpenCreditLinesAndLoans',\n",
    "                                                 # 'NumberRealEstateLoansOrLines', \n",
    "                                                 'Code98'])\n",
    "\n",
    "# import seaborn as sns\n",
    "# sns.heatmap(train_data_no_label.corr(), annot = True,annot_kws={'size': 6})\n",
    "\n",
    "# vif_data = pd.DataFrame()\n",
    "# vif_data['feature'] = train_data_no_label.columns\n",
    "# vif_data['VIF'] = [variance_inflation_factor(train_data_no_label.values, i) for i in range(train_data_no_label.shape[1])]\n",
    "# print(vif_data)\n",
    "\n",
    "def vif_and_corr_check(df):\n",
    "    \n",
    "    import seaborn as sns\n",
    "    sns.heatmap(df.corr(), annot = True,annot_kws={'size': 6})\n",
    "\n",
    "    from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "    df_no_label = df.drop(columns = ['SeriousDlqin2yrs'])\n",
    "    vif_data = pd.DataFrame()\n",
    "    vif_data['feature'] = df_no_label.columns\n",
    "    vif_data['VIF'] = [variance_inflation_factor(df_no_label.values, i) for i in range(df_no_label.shape[1])]\n",
    "    print(vif_data)\n",
    "    \n",
    "vif_and_corr_check(train_data_no_label)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "a36f5d8d-c474-4542-a393-fc716373f206",
   "metadata": {},
   "source": [
    "'''\n",
    "age, MonthlyIncome, NumberOfOpenCreditLinesAndLoans, \n",
    "NumberRealEstateLoansOrLines, DebtPayments, \n",
    "Code98, PastDueRiskScore  \n",
    "У всех высокая мультиколлинеарность\n",
    "\n",
    "Code98 - коррелирует с PastDueRiskScore, потому дропаем Code98, эти флаги нужны были только для Kaggle скора, в проде их не будет\n",
    "для остальных попробуем биннинг\n",
    "'''"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "bb464a64-08a7-4c33-a092-09d98ed68915",
   "metadata": {},
   "source": [
    "### Биннинг "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "d95e03b6-e26a-4d4f-a226-cd45c3773ab8",
   "metadata": {},
   "source": [
    "# Биннинг\n",
    "\n",
    "# NumberOfOpenCreditLinesAndLoans\n",
    "train_data['ConsumerCredit_Group'] = pd.cut(train_data['NumberOfOpenCreditLinesAndLoans'], \n",
    "                                bins = [0,1, 2,6,15,31], \n",
    "                                labels=[\n",
    "                                    '0_loans',\n",
    "                                    '1_loans',\n",
    "                                    '2-5_loans',\n",
    "                                    '6-14_loans',\n",
    "                                    '16-30_loans'\n",
    "                                ])\n",
    "consumer_dummy = pd.get_dummies(train_data['ConsumerCredit_Group'], prefix='Consumer', drop_first = False).astype('float')\n",
    "\n",
    "train_data['RealEstateLoans_Group'] = pd.cut(train_data['NumberRealEstateLoansOrLines'],\n",
    "                                   bins=[-1, 0, 3,100], \n",
    "                                   labels= [\n",
    "                                            '0_loans',      \n",
    "                                            '1-3_loans',    \n",
    "                                            '4+_loans',    \n",
    "                                            ])\n",
    "estate_dummy = pd.get_dummies(train_data['RealEstateLoans_Group'], prefix='RealEstateLoans', drop_first = False).astype('float')\n",
    "\n",
    "train_data = pd.concat([train_data, consumer_dummy, estate_dummy], axis = 1).copy()\n",
    "train_data = train_data.drop(columns = ['ConsumerCredit_Group', \n",
    "                                        'RealEstateLoans_Group']).copy()\n",
    "\n",
    "# За базовые категории взяты Consumer_6-14_loans тк является самой частой и с наименьшим риском, те за базу взять \"идеальный клиент\"\n",
    "# И RealEstateLoans_0_loans, тк является самой частой категорией + сравнение относительно нуля\n",
    "train_data = train_data.drop(columns = ['Consumer_6-14_loans',  \n",
    "                                        'RealEstateLoans_0_loans']).copy()\n",
    "\n",
    "# age\n",
    "# age_bins = [18, 31, 41, 51, 61, 71, 81]\n",
    "# age_bins_labels = ['18-30', '31-40', '41-50', '51-60', '61-70', '71-80']\n",
    "# train_data['age_bins'] = pd.cut(\n",
    "#     x = train_data['age'],\n",
    "#     bins = age_bins,\n",
    "#     labels = age_bins_labels,\n",
    "#     right = False,\n",
    "#     include_lowest = True)\n",
    "# train_data['age_bins'] = train_data['age_bins'].astype('category')\n",
    "\n",
    "# test_data['age_bins'] = pd.cut(\n",
    "#     x = test_data['age'],\n",
    "#     bins = age_bins,\n",
    "#     labels = age_bins_labels,\n",
    "#     right = False,\n",
    "#     include_lowest = True)\n",
    "# test_data['age_bins'] = test_data['age_bins'].astype('category')\n",
    "\n",
    "\n",
    "train_data"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "98dacfa8-48d1-4bf6-9d28-b59b21780f26",
   "metadata": {},
   "source": [
    "train_data_no_label = train_data.drop(columns = [ \n",
    "                                                 'NumberOfOpenCreditLinesAndLoans',\n",
    "                                                 'NumberRealEstateLoansOrLines', \n",
    "                                                 #'MonthlyIncome_over_20k', \n",
    "                                                 #'NumberRealEstateLoansOrLines_over_5',\n",
    "                                                 'RealEstateLoans_1-3_loans',\n",
    "                                                 'DebtRatio',\n",
    "                                                 'MonthlyIncomeIsMissing',\n",
    "                                                 'Code98'])\n",
    "vif_and_corr_check(train_data_no_label)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "f08cbd2b-dc71-43c9-8779-e0b4048bfef0",
   "metadata": {},
   "source": [
    "bins = [18, 30, 40,50,60,70,80,81]\n",
    "interval_check(train_data, 'age', bins)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a589f9b-4ae7-44d1-9b87-c4e36a6331cf",
   "metadata": {},
   "source": [
    "# from sklearn.inspection import permutation_importance \n",
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# importance_df = train_data.drop(columns = ['SeriousDlqin2yrs', \n",
    "#                                       'age', \n",
    "#                                       'DebtPayments', \n",
    "#                                       #'DebtPaymentsClipped', \n",
    "#                                       'DebtPayments_below_3k',\n",
    "#                                       'DebtPayments_between_3k_9k',\n",
    "#                                       #'DebtPayments_over_9k'\n",
    "#                                      ])\n",
    "\n",
    "# rfc = RandomForestClassifier(n_estimators = 100, max_depth = 5, class_weight = 'balanced').fit(importance_df, train_data['SeriousDlqin2yrs'])\n",
    "# perm_result = permutation_importance (rfc, \n",
    "#                                       X = importance_df, \n",
    "#                                       y = train_data['SeriousDlqin2yrs'],\n",
    "#                                       n_repeats = 5,\n",
    "#                                       random_state = 42)\n",
    "# print('Permutation Importance')\n",
    "# for feature, importance_mean, importance_std in zip(importance_df.columns,\n",
    "#                                                                    perm_result.importances_mean,\n",
    "#                                                                    perm_result.importances_std):\n",
    "#     print(f'{feature}: {importance_mean} +- {importance_std}')"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80be10d7-e93a-4f29-9231-7e9c1dd73b50",
   "metadata": {},
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "\n",
    "importance_df = train_data.drop(columns = ['SeriousDlqin2yrs',\n",
    "                                           'NumberOfOpenCreditLinesAndLoans',\n",
    "                                                 'NumberRealEstateLoansOrLines', \n",
    "                                           'RealEstateLoans_1-3_loans',\n",
    "                                                 #'DebtRatio',\n",
    "                                                 'MonthlyIncomeIsMissing',\n",
    "                                                 'Code98',\n",
    "                                                   'Code96',\n",
    "                                                    'MonthlyIncome_over_20k',\n",
    "                                                    'Consumer_0_loans',\n",
    "                                                    'NumberOfOpenCreditLinesAndLoans_over_30'\n",
    "                                                   ])\n",
    "\n",
    "importance_df['DebtRatio'] = importance_df['DebtRatio'].clip(0,10).copy()\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    importance_df,\n",
    "    train_data['SeriousDlqin2yrs'],\n",
    "    test_size=0.3,\n",
    "    random_state=42,\n",
    "    stratify = train_data['SeriousDlqin2yrs']\n",
    ")\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "\n",
    "X_train_scaled_df = pd.DataFrame(X_train_scaled, columns = X_train.columns, index = X_train.index)\n",
    "X_val_scaled_df = pd.DataFrame(X_val_scaled, columns = X_val.columns, index = X_val.index)\n",
    "\n",
    "scale_pos_weight = len(y_train[y_train == 0]) / len(y_train[y_train == 1])\n",
    "results = {}\n",
    "\n",
    "models = {\n",
    "    'LogisticRegression': LogisticRegression(\n",
    "        max_iter=1000,\n",
    "        class_weight = 'balanced',\n",
    "        random_state = 42   \n",
    "    ),\n",
    "    \n",
    "    'RandomForest': RandomForestClassifier(\n",
    "        n_estimators=100, \n",
    "        max_depth=5,\n",
    "        class_weight = 'balanced',\n",
    "        random_state = 42\n",
    "    ),\n",
    "    \n",
    "    'XGBoost': XGBClassifier(\n",
    "        n_estimators=100, \n",
    "        max_depth=3,\n",
    "        scale_pos_weight = scale_pos_weight,\n",
    "        eval_metric = 'logloss',\n",
    "        random_state = 42\n",
    "    )\n",
    "}\n",
    "\n",
    "\n",
    "print(\"ROC-AUC:\")\n",
    "for model_name, model in models.items():\n",
    "    if model_name == 'LogisticRegression':\n",
    "        X_train_use = X_train_scaled_df\n",
    "        X_val_use = X_val_scaled_df\n",
    "        \n",
    "    else:\n",
    "        X_train_use = X_train\n",
    "        X_val_use = X_val\n",
    "        \n",
    "    model.fit(X_train_use, y_train)\n",
    "    y_pred = model.predict_proba(X_val_use)[:,1]\n",
    "    roc_auc = roc_auc_score(y_val, y_pred)\n",
    "    results[model_name] = roc_auc\n",
    "    print(f'{model_name}: {roc_auc:.4f}')\n",
    "\n",
    " # Permutation Importance\n",
    "\n",
    "importance_xgb = XGBClassifier(\n",
    "        n_estimators=100, \n",
    "        max_depth=3,\n",
    "        scale_pos_weight = scale_pos_weight,\n",
    "        eval_metric = 'logloss',\n",
    "        random_state = 42)\n",
    "\n",
    "importance_xgb.fit(X_train, y_train)\n",
    "    \n",
    "\n",
    "perm_result = permutation_importance(\n",
    "    importance_xgb,\n",
    "    X_val, y_val,\n",
    "    n_repeats = 10,\n",
    "    scoring = 'roc_auc',\n",
    "    random_state = 42)\n",
    "\n",
    "perm_importance_df = pd.DataFrame({\n",
    "    'feature': X_val.columns,\n",
    "    'perm_importance': perm_result.importances_mean,\n",
    "    'perm_std': perm_result.importances_std\n",
    "}).sort_values('perm_importance', ascending = False)\n",
    "\n",
    "print('Permutation Importance')\n",
    "print(perm_importance_df)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "280e8ca2-fc92-48d5-b1e3-f18f93e4b836",
   "metadata": {},
   "source": [
    "## Scaling Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d5f87b4-ad3b-4423-ac02-975d93802dc1",
   "metadata": {},
   "source": [
    "# train_data.nunique()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4f114ed-a69d-4271-90fb-26978bcc69b0",
   "metadata": {},
   "source": [
    "# from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# # boolean_columns = [\n",
    "# #     'Consumer_0-1_loans',\n",
    "# #     'Consumer_2-3_loans',\n",
    "# #     'Mortgage_1-3_loans',\n",
    "# #     'age_18-30',\n",
    "# #     'age_31-40',\n",
    "# #     'age_41-50',\n",
    "# #     'age_51-60',\n",
    "# #     'age_61-70',\n",
    "# #     'age_71-80'\n",
    "# #     'DebtPayments_over_9k']\n",
    "\n",
    "# boolean_columns = [\n",
    "#     'Consumer_0-1_loans',\n",
    "#     'Consumer_2-3_loans',\n",
    "#     'Mortgage_1-3_loans',\n",
    "#     'DebtPayments_over_9k']\n",
    "\n",
    "# scaler = StandardScaler()\n",
    "# scaler_train = scaler.fit(train_data.drop(boolean_columns, axis = 1))\n",
    "# train_data_scaled = scaler_train.transform(train_data.drop(boolean_columns, axis = 1))\n",
    "# test_data_scaled = scaler_train.transform(test_data.drop(boolean_columns, axis = 1))\n",
    "\n",
    "# train_data_scaled_df = pd.concat([pd.DataFrame(train_data_scaled),\n",
    "#                                   train_data[boolean_columns].reset_index(drop = True)], axis = 1)\n",
    "\n",
    "# train_data_scaled_df.columns = train_data.columns\n",
    "\n",
    "# test_data_scaled_df = pd.concat([pd.DataFrame(test_data_scaled),\n",
    "#                                   test_data[boolean_columns].reset_index(drop = True)], axis = 1)\n",
    "\n",
    "# test_data_scaled_df.columns = test_data.columns\n",
    "\n",
    "# train_data_scaled_df"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0857882-89ef-48d3-aa50-2fb80c8a1c84",
   "metadata": {},
   "source": [
    "# train_data_scaled_df.to_csv(FOLDER_PATH + 'data/train_VIF_EDA_cont_age_v2.csv')\n",
    "# test_data_scaled_df.to_csv(FOLDER_PATH + 'data/test_VIF_EDA_cont_age_v2.csv')\n",
    "# train_label.to_csv(FOLDER_PATH + '/data/train_label.csv')"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f651f33-927b-449b-8e88-0120262a9bf8",
   "metadata": {},
   "source": [],
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
